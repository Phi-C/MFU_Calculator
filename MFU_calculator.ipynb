{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48850142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mma_flops(m, n, k, bias=False):\n",
    "    \"\"\"\n",
    "    Calculate the number of floating point operations (FLOPs) for MMA operation.\n",
    "\n",
    "    Args:\n",
    "        [m, k] x [k, n] -> [m, n]:\n",
    "        m (int): Number of rows in the first matrix.\n",
    "        n (int): Number of columns in the second matrix.\n",
    "        k (int): Number of columns in the first matrix (and rows in the second matrix).\n",
    "        bias (bool): Whether a bias term is included.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of FLOPs.\n",
    "    \"\"\"\n",
    "    flops = m * n * (2 * k - 1)  # Each output element requires k multiplications and (k - 1) additions\n",
    "\n",
    "    # ouput: [m, n]\n",
    "    # bias:  [n]\n",
    "    if bias:\n",
    "        flops += m * n           # Adding bias requires m * n additions\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3a33d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_flops(m, n):\n",
    "    \"\"\"\n",
    "    Calculate the number of floating point operations (FLOPs) for a softmax operation.\n",
    "\n",
    "    Args:\n",
    "        m (int): Number of rows.\n",
    "        n (int): Number of columns.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of FLOPs.\n",
    "    \"\"\"\n",
    "    # Take one row as example\n",
    "    # Exponentiation: n\n",
    "    # Sum: n - 1\n",
    "    # Division: n\n",
    "    flops = (3 * n - 1)  * m\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea8496ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_transfrom_flops(batch_size, seq_len, hidden_dize, bias=False):\n",
    "    \"\"\"\n",
    "    Calculate the number of floating point operations (FLOPs) for a linear transformation.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of samples in the batch.\n",
    "        seq_len (int): Length of the sequence (number of input features).\n",
    "        hidden_dize (int): Number of output features.\n",
    "        bias (bool): Whether a bias term is included.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of FLOPs.\n",
    "    \"\"\"\n",
    "    return mma_flops(batch_size * seq_len, hidden_size, hidden_size, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3846420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mha_gqa_flops(batch_size, seq_len, hidden_size, head_dim, num_heads, num_kv_heads):\n",
    "    \"\"\"\n",
    "    Calculate the number of floating point operations (FLOPs) for multi-head attention.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of samples in the batch.\n",
    "        seq_len (int): Length of the sequence.\n",
    "        hidden_size (int): Size of the hidden layer.\n",
    "        head_dim (int): Dimension of each attention head.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        num_kv_heads (int): Number of key-value heads.\n",
    "\n",
    "    Note:\n",
    "        The number of key-value heads is usually equal to the number of attention heads,\n",
    "        but can be different in some architectures.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of FLOPs.\n",
    "    \"\"\"\n",
    "    # Query projection\n",
    "    q_proj = mma_flops(batch_size * seq_len, head_dim * num_heads, hidden_size, bias=False)\n",
    "    # Key projection and Value projection\n",
    "    kv_proj = 2 * mma_flops(batch_size * seq_len, head_dim * num_kv_heads, hidden_size, bias=False)\n",
    "\n",
    "    # Attention scores: batch_size * num_heads * query_len * kv_len\n",
    "    attn_score = mma_flops(batch_size * seq_len, batch_size * seq_len, head_dim) * num_heads\n",
    "\n",
    "    # mask_fill整个attention score矩阵, 每个元素都做一次判断, 对其中若干个元素进行赋值操作（这里忽略）\n",
    "    mask_fill = batch_size * num_heads * (seq_len * seq_len)\n",
    "    softmax = softmax_flops(seq_len, seq_len) * batch_size * num_heads\n",
    "\n",
    "    # Attention: [seq_len, seq_len] * [seq_len, head_dim] for one batch and one head ---> [bs * seq_len, head_dim * num_heads]\n",
    "    attn_score_v = batch_size * mma_flops(seq_len, seq_len, head_dim) * num_heads\n",
    "\n",
    "    # Output projection\n",
    "    attn_out = mma_flops(batch_size * seq_len, head_dim * num_heads, hidden_size, bias=False)\n",
    "\n",
    "    flops = q_proj + kv_proj + attn_score + mask_fill + softmax + attn_score_v + attn_out\n",
    "    return flops"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
