{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48850142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mma_flops(m, n, k, bias=False):\n",
    "    \"\"\"\n",
    "    Calculate the number of floating point operations (FLOPs) for MMA operation.\n",
    "\n",
    "    Args:\n",
    "        [m, k] x [k, n] -> [m, n]:\n",
    "        m (int): Number of rows in the first matrix.\n",
    "        n (int): Number of columns in the second matrix.\n",
    "        k (int): Number of columns in the first matrix (and rows in the second matrix).\n",
    "        bias (bool): Whether a bias term is included.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of FLOPs.\n",
    "    \"\"\"\n",
    "    flops = m * n * (2 * k - 1)  # Each output element requires k multiplications and (k - 1) additions\n",
    "\n",
    "    # ouput: [m, n]\n",
    "    # bias:  [n]\n",
    "    if bias:\n",
    "        flops += m * n           # Adding bias requires m * n additions\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3a33d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_flops(m, n):\n",
    "    \"\"\"\n",
    "    Calculate the number of floating point operations (FLOPs) for a softmax operation.\n",
    "\n",
    "    Args:\n",
    "        m (int): Number of rows.\n",
    "        n (int): Number of columns.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of FLOPs.\n",
    "    \"\"\"\n",
    "    # Take one row as example\n",
    "    # Exponentiation: n\n",
    "    # Sum: n - 1\n",
    "    # Division: n\n",
    "    flops = (3 * n - 1)  * m\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8496ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_transfrom_flops(batch_size, seq_len, hidden_dize, bias=False):\n",
    "    \"\"\"\n",
    "    Calculate the number of floating point operations (FLOPs) for a linear transformation.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of samples in the batch.\n",
    "        seq_len (int): Length of the sequence (number of input features).\n",
    "        hidden_dize (int): Number of output features.\n",
    "        bias (bool): Whether a bias term is included.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of FLOPs.\n",
    "    \"\"\"\n",
    "    return mma_flops(batch_size * seq_len, hidden_size, hidden_size, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3846420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mha_gqa_flops(batch_size, seq_len, hidden_size, head_dim, num_heads, num_kv_heads):\n",
    "    \"\"\"\n",
    "    Calculate the number of floating point operations (FLOPs) for multi-head attention.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of samples in the batch.\n",
    "        seq_len (int): Length of the sequence.\n",
    "        hidden_size (int): Size of the hidden layer.\n",
    "        head_dim (int): Dimension of each attention head.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        num_kv_heads (int): Number of key-value heads.\n",
    "\n",
    "    Note:\n",
    "        The number of key-value heads is usually equal to the number of attention heads,\n",
    "        but can be different in some architectures.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of FLOPs.\n",
    "    \"\"\"\n",
    "    # Query projection\n",
    "    q_proj = mma_flops(batch_size * seq_len, head_dim * num_heads, hidden_size, bias=False)\n",
    "    # Key projection and Value projection\n",
    "    kv_proj = 2 * mma_flops(batch_size * seq_len, head_dim * num_kv_heads, hidden_size, bias=False)\n",
    "\n",
    "    # Attention scores: batch_size * num_heads * query_len * kv_len\n",
    "    attn_score = mma_flops(batch_size * seq_len, batch_size * seq_len, head_dim) * num_heads\n",
    "\n",
    "    # mask_fill整个attention score矩阵, 每个元素都做一次判断, 对其中若干个元素进行赋值操作（这里忽略）\n",
    "    mask_fill = batch_size * num_heads * (seq_len * seq_len)\n",
    "    softmax = softmax_flops(seq_len, seq_len) * batch_size * num_heads\n",
    "\n",
    "    # Attention: [seq_len, seq_len] * [seq_len, head_dim] for one batch and one head ---> [bs * seq_len, head_dim * num_heads]\n",
    "    attn_score_v = batch_size * mma_flops(seq_len, seq_len, head_dim) * num_heads\n",
    "\n",
    "    # Output projection\n",
    "    attn_out = mma_flops(batch_size * seq_len, head_dim * num_heads, hidden_size, bias=False)\n",
    "\n",
    "    flops = q_proj + kv_proj + attn_score + mask_fill + softmax + attn_score_v + attn_out\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2519106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn_flops(batch_size, seq_len, intermediate_size, hidden_size, bias=False):\n",
    "    \"\"\"\n",
    "    Calculate the number of floating point operations (FLOPs) for a feed-forward network (FFN).\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of samples in the batch.\n",
    "        seq_len (int): Length of the sequence.\n",
    "        intermediate_size (int): Size of the intermediate layer.\n",
    "        hidden_size (int): Size of the hidden layer.\n",
    "        bias (bool): Whether a bias term is included.\n",
    "\n",
    "    Note:\n",
    "        The typical FFN is implemented as:\n",
    "            >> class DeepseekV3MLP(nn.Module):\n",
    "            >>     def __init__(self, config, hidden_size=None, intermediate_size=None):\n",
    "            >>         super().__init__()\n",
    "            >>         self.config = config\n",
    "            >>         self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n",
    "            >>         self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n",
    "\n",
    "            >>         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "            >>         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "            >>         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "            >>         self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "            >>     def forward(self, x):\n",
    "            >>         down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "            >>         return down_proj\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of FLOPs.\n",
    "    \"\"\"\n",
    "    # First linear transformation\n",
    "    gate_proj_flops = mma_flops(batch_size * seq_len, intermediate_size, hidden_size, bias)     # [m, intermediate_size]\n",
    "    # Second linear transformation\n",
    "    up_proj_flops = mma_flops(batch_size * seq_len, intermediate_size, hidden_size, bias)       # [m, intermediate_size]\n",
    "    elementwise_mul_flops = batch_size * seq_len * intermediate_size                            # Element-wise multiplication\n",
    "\n",
    "    # Third linear transformation\n",
    "    down_proj_flops = mma_flops(batch_size * seq_len, hidden_size, intermediate_size, bias)     # [m, hidden_size]\n",
    "\n",
    "    return gate_proj_flops + up_proj_flops + elementwise_mul_flops + down_proj_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c415794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfu_calculation(flops,\n",
    "                    step_time,\n",
    "                    gpu_num,\n",
    "                    gpu_flops):\n",
    "    \"\"\"\n",
    "    Calculate the Model FLOPS Utilization (MFU).\n",
    "\n",
    "    Args:\n",
    "        flops (int): Total number of floating point operations. [FLOPs]\n",
    "        step_time (float: ms): Time taken for the operation in microseconds. [ms]\n",
    "        gpu_num (int): Number of GPUs used. [1]\n",
    "        gpu_flops (float): The theoretical peak FLOPs of a single GPU. [TFLOPs/s]\n",
    "            A100: FP32: 19.5 TFLOPs/s  BF16  FP16: 312 TFLOps/s \n",
    "    Returns:\n",
    "        float: Model FLOPS Utilization (MFU) as a percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    return flops / (10 ** 12 * gpu_flops * gpu_num * step_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
